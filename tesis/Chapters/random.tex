\chapter{Probabilistic Algorithms}
\label{sec:prob}


In this chapter we consider probabilistic algorithms for SAT and $k$-SAT.When we talk about probabilistic algorithms, we are trying to define an incomplete SAT-solver, with a bounded probability error. This might seems like a big loss in power. Nonetheless, given the complexity of the problem, neither are complete solvers capable of solving all formulas in a feasible time. Therefore, dropping completeness could be a fair exchange in order to get better time complexity.\\


\section{Paturi-Pudlák-Zane}
The first one that we will consider is the Paturi-Pudlák-Zane(PPZ) algorithm \cite{paturi1997satisfiability} developed in 1997 and its improvements Paturi-Pudlák-Saks-Zane(PPSZ). It was the first probabilistic algorithm for $k$-SAT proven to work. It has an associated deterministic version that could well be included in the DPLL chapter. Then, some improvements have been done to the algorithm in \cite{paturi2005improved} and \cite{hertli20143}.\\

\subsection{Paturi-Pudlák-Zane}
\label{subsec:PPZ}
In this subsection we will present the PPZ algorithm and in the next subsection its improved version PPSZ. The information presented here follows the discussion in \cite{paturi2005improved}. The difference between PPZ and PPSZ is some added preprocessing. At the time of release, PPSZ was the asymptotically fastest algorithm for random $k$-SAT with $k \ge 4$ only improved in $3$-SAT by the Schönning random walk algorithm and its improved version the Hofmeister algorithm, because PPSZ were not able to extend the results they found but it was suggested that it should be extendable. At the end, it was proved 9 years later by Hertli \cite{hertli20143} that the bounds hold on general. \\


To define the algorithms, we first define some subroutines. The first of them take a CNF formula $F$, an assignment $\alpha$ and a permutation $\pi$ and returns other assignment $u$.Note that in line 5 and 7 on the procedure \texttt{modify}[Algorithm\ref{alg:modify}]is only checking whether or not we can unit propagate the variable $x_{\pi(i)}$. The algorithm \texttt{Search}[Algorithm \ref{search}] is obtained by running \texttt{Modify} on many pairs $(\alpha, \pi)$ where $\alpha$ is a random assignment and $\pi$ a random permutation.\\ 

\begin{algorithm}
  \caption{\texttt{Modify subroutine}}\label{alg:modify}
  \begin{algorithmic}[1]
    \Procedure{\texttt{Modify}}{$\alpha, \pi$, $F$}
    \State $F_0 \gets F$
    \State $u \gets$ empty partial assignment. 
    \State
    \For{$i \in \{0,...,m-1\}$}
    \If{$\{x_{\pi(i)}\} \in F_i$}
    \State $u += \{x_{\pi(i)} = 1\}$
    \Else \If{$\{\neg x_{\pi(i)}\} \in F_i$}
    \State $u += \{x_{\pi(i)} = 0\}$
    \Else
    \State $u += \{x_{\pi(i)} = \alpha(x_{\pi(i)})\}$
    \EndIf
    \EndIf
    \EndFor    
    \State $F_{i+1} = F_iu $
    \State \Return u
    \EndProcedure
  \end{algorithmic}
\end{algorithm}




This procedure is the named PPZ algorithm. As we can see is a pretty simple algorithm, but more often than not the work on random algorithms is not to program but to prove them correct. Therefore we will proceed to prove why this algorithm is, in fact, a correct probabilistic algorithm.\\


\begin{algorithm}
  \caption{\texttt{Search subroutine}}\label{search}
  \begin{algorithmic}[1]
    \Procedure{\texttt{Search}}{$F$, $I$}
    \For{$i \in \{0,...,I\}$}
    \State $\alpha\gets$   random assignment on $Var(F)$
    \State $\pi\gets$    random permutation on $1,...,|Var(F)|$
    \State $u\gets$ \texttt{Modify}($\alpha, \pi, F$)
    \If{$u(F) = 1$}
    \State \Return Satisfiable
    \EndIf
    \EndFor
    \State \Return Unsatisfiable
    \EndProcedure
  \end{algorithmic}
\end{algorithm}






\texttt{Search} always answers Unsatisfiable if $F$ is unsatisfiable. The only problem is to upper bound the error probability in the case that $F$ is unsatisfiable. In fact, we only have to to find $\tau(F)$: the probability that \texttt{Modify}($F,\pi, \alpha$) find a satisfying assignment. The error probability  of search would be therefore $(1-\tau(F))^{I}$. As $1-x \le exp(-x)$ with $x \in [0,1]$ them $(1-\tau(F))^{I} \le exp(-I\tau(F))$, which is at most $exp(-n)$ where $n= |Var(F)|$ provided  $I>n/\tau(F)$ . it suffices to give good upper bounds on $\tau(F)$. In order to do that we will prove first two lemmas.\\

To prove the first lemma we introduce some notation:

\begin{definition}
  A variable $x$ is forced for an assignment $\alpha$, a formula $F$ and a permutation $\pi$ if $x$ is unit propagated in the procedure \texttt{Modify}$(\alpha, \pi, F)$. $Forced(\alpha, \pi, F)$ is the set of all variables that are forced for $(\alpha, \pi, F)$
\end{definition}

\begin{lemma} Let $z$ be a satisfying assignment of a CNF formula $G$, and let $\pi$ be a permutation of 
$\{1,...,n\}$ and $y$ be any assignment to the variables. Then,
  \texttt{Modify}( $G,\pi,y$)=$z$ if and only if $y(x) = z(x)\ \  \forall x \in Var(G) \backslash \text{Forced}(z, \pi, G)$ .
\end{lemma}

\begin{proof}
  
  If $y(x) = z(x)\ \  \forall x \in Var(G) \backslash \text{Forced}(z, \pi, G)$ we prove that $u$ = $z$ where $u$ is the assignment provided by  \texttt{Modify}($i,\pi,F$). by induction on $i$.  $x_\{\pi(0)\}$ is forced only if $F$ has a unit clause on $x$, therefore either it is forced for all assignments or it is not forced for any of them. Otherwise $u(x_{\pi(0)}) = z(x_{\pi(0)})= y(x_{\pi(0)})$ Therefore $u(x_{\pi(0)}) = z(x_{\pi(0)})$. Let suppose that $u(x_{\pi(j)}) = z(x_{\pi(j)})$ for $j < i$. If the variable $x_{\pi(i)}$ is forced on $z$ it should be forced on $u$ to (and to the same value). Otherwise $u(x_{\pi(j)}) = z(x_{\pi(j)})= y(x_{\pi(j)})$. \\

  Let $i$ be the first index such that $y(x_{\pi(i)})\ne z(x_{\pi(i)})$ with $x_{\pi(i)} \not \in \text{Forced}(z, \pi, G)$ therefore $u(x_{\pi(i)})=y(x_{\pi(i)})\ne z(x_{\pi(i)})$.
\end{proof}

Now, let $\tau (F,z)$ the probability that \texttt{Modify}$(\alpha,\pi,F)$ would return $z$ with random $\pi$ and $\alpha$. From the previous lemma:

$$ \tau(F,z) = 2^{-n} \mathbb{E}_{\pi}[2^{|\text{Forced}(z, \pi, F)|}] \ge^{1.} 2^{-n +E_{\pi}[{|\text{Forced}(z, \pi, F)|}] }$$

Where 1. is by the convexity of the exponential function and $\mathbb{E}_{\pi}$ is the expected value with $\pi$ as variable. \\

Let $v$ be a variable in $Var(f)$ and $z$ a satisfying assignment of $F$. let $C$ be a clause in $F$, then we say that $C$ is critical for $(v,z,F)$ if the only true literal in $C$ is the one corresponding to $v$. Suppose that $\pi$ is a permutation such that $v$ appears after all other variables in $C$. It is easy to follow that $v \in \text{Forced}(z,\pi,F)$ if $C$ is critical for $(v,z,F)$. Conversely, if $z$ is forced it must be critical and appears last on the permutation. Let $Last(v,G,z)$ be the set of permutation of the variables such that for at least one critical clause for $(v,G,z)$, v appears last on the permutation. That is, the set of permutations where $v$ is forced. Let $P(v,z,F)$ the probability that a random permutation is in $Last(v,G,z)$. It follows that

$$ \mathbb{E}_{\pi}[|\text{Forced}(z, \pi, F)|]= \sum_{v \in Var(F)} \mathbb{E}_{\pi}[v \in \text{Forced}( z, \pi, F)] = \sum_{v \in Var(F)} P(v, z, F)  $$

Putting it all together we have:

\begin{lemma}
  \label{labelito}
  For any satisfying assignment $z$ of a CNF formula $F$
  $$\tau(F,z) \ge 2^{-n + \sum_{v \in Var(F)} P(v, z, F)}$$

  In particular, if $P(v,z,F) \ge p$ for all variables $v$ then $\tau(G,z) \ge 2^{-(1-p)n}$.
\end{lemma}


\begin{theorem}
  Let $F$ be a $k$-CNF formula. If $F$ is satisfiable by an isolated assignment, $\tau(F) \ge 2^{-(1-\frac{1}{k})n}$, where $n$ is the number of variables.
\end{theorem}
\begin{proof}
  Let $z$ be a satisfying assignment of $F$. Then $\tau(F) \ge \tau(F,z)$. If $z$ is an isolated assignment, them for each variable $v$ there is a critical clause $C_v$ and the probability that for a random permutation $v$ appear last is $1/k$. Therefore by the previous lemma  $$\tau(F) \ge \tau(F,z) \ge 2^{-(1-\frac{1}{k})n}$$
\end{proof}


Then we can think that it is unusual that it is easier to guess a satisfying assignment with such a simple method when there is less satisfiable assignments. We are now going to formalize that intuition, growing on the previous lemmas, and giving similar arguments. For that we will introduce a new concept.

\begin{definition}

  Let $\alpha$ be an assignment of a proper subset $A \subset Var(F)$. Then the subcube defined by $\alpha$ is the set of the assignments that extends $\alpha$, i.e. all $\beta$ that assign all elements in $Var(F)$ and $\beta(x)=\alpha(x), \forall x \in A$.
\end{definition}

\begin{lemma}
  Let $V$ be a set of variables and let $A\ne \emptyset$ be a set of assignments that map all variables in $V$. The set of all assignments that map all $V$ can be partitioned into a family $(B_z : z \in A)$ of distinct disjoint subcubes so that $z \in B_z \ \forall z \in A$.
\end{lemma}

\begin{proof}
  If $|A|=1$ choose $B_z$ as the set of all possible assignments. Otherwise there is two assignments that differ on one variable $X$. We will partition two subcubes: the one from the assignment that map $x$ to 0 and the assignment that map $x$ to 1. Then we proceed recursively on both subcubes.
\end{proof}


Given a formula $F$ we will apply this lemma to the set $sat(F)$ of assignments that satisfy $F$, and obtain a family of $\{B_z : z \in sat(F)\}$. We will analyze the probability $\tau(F, z | B_z)$, that is, the probability of \texttt{Modify}$(y, \pi, F)$ returns $z$ given that $y \in B_z$. It is easy to follow that:



\begin{align*}
  \tau(G) \ge \sum_{z\in sat(F)}  \tau(G, z | B_z) Prob(y \in B_z)  &\ge  \sum_{z\in sat(F)} \min_{\chi \in sat(F)} \{\tau(G, \chi | B_\chi)\}  Prob(y \in B_z)\\ & = \min_{\chi \in sat(F)} \{\tau(G, \chi | B_\chi)\}
\end{align*}


Further on let $z$ be a satisfying assignment and $B = B_z$. Let $N$ be the set of unassigned variables in $B_z$ (the set of variables that are not assigned equal for all $\alpha$ in $B$). Writing $Forced_z (y,\pi,F) = N \cap Forced(y,\pi,F)$  we have

$$\tau(F, z | B) \ge 2^{-N + E[|Forced_z(z, \pi, G|]}$$

Therefore $P(v,z,F) \ge 1/k$ for $v\in N$. This is true because z is the unique satisfying assignment in $B$, hence changing the value in $v$ produce a nonsatisfying assignment. Therefore $v$ is critical on some permutation and analogously as the lemma\ref{labelito} we have that $P(v,z,F)$.

\begin{theorem}[\cite{paturi2005improved}]  Let $F$ be a $k$-CNF formula, $z$ a satisfying assignment and let $B$ be a subcube on $Var(F)$ that contains $z$ and no other satisfying assignment. Then:

  $$ \tau(G, z|B) \ge 2^{-(1-\frac{1}{k})|N|}$$
\end{theorem}


With that we could analyze the complexity of this algorithm. \texttt{Modify} run on $O(nC)$ where $n$ is the number of variables and $C$ is the number of clauses (assign CNF-formula has a worst case of $C$).  \texttt{Search} run on $O(I\cdot O(\text{Modify}))$ supposing that we can get a random number in $O(1)$ and therefore a random assignment and a  random permutation on $O(n)$. As we will set $I> n/\tau(G,z|B)>n/\tau(F)$ we get a running time of $O(n\cdot C\cdot 2^{1-\frac{1}{k}n})$, with a one-sided error probability of $e^{-n}$ (0.049 for 3-SAT).


\subsection{Paturi-Pudlák-Saks-Zane}

This algorithm includes a preprocessing of the formula prior to the searching algorithm. This preprocessing will try to find isolated assignments improving its running time (or at least its complexity analysis). The preprocessing takes as input a CNF formula $F$ and a positive integer $I$. It uses the concept of resolution: should it happen that we have to clauses $C_1 = \{x_1,...,x_n\}$, $C_2 = \{y_1,...,y_{n'}\}$  such that $C_1,C_2 \in G$ and the literal $x_i = \neg y_j; \ i\in\{1,...,n\},\ j \in \{1,...,n'\} $ them we could generate a clause $C = R(C_1,C_2) = \{x_k : k \in\{1,...,n\}\backslash i\} \cup \{y_k : k \in\{1,...,n'\}\backslash j\}$ and the formula $F' = F \wedge C$ has the same satisfying assignment that $F$. A pair of clauses $(C_1,C_2)$ are said to be s-bounded if they are resolvable and $|C_1|,|C_2|, |R(C_1,C_2)| < s$.

\begin{algorithm}
  \caption{Resolve subroutine}\label{alg:resolve}
  \begin{algorithmic}[1]
    \Procedure{\texttt{Resolve}}{$F, s$}
    \State $F_s = F$
    \While{$F_s$ has a s-bounded pair $(C_1,C_2)$ with $R(C_1,C_2) \not \in F_s$}
    \State $F_s  = F_s \wedge R(C_1, C_2)$
    \EndWhile
    \State \Return Satisfiable
    \State \Return $F_s$
    \EndProcedure
    \State
    \Procedure{\texttt{ResolveSat}}{$F, s, I$}
    \State $F_s =$ \texttt{Resolve}$(F,s)$
    \State \Return \texttt{Search}$(F,s)$
	\EndProcedure
  \end{algorithmic}
\end{algorithm}

With this prepossessing added to the algorithm a better upper bound is proved. Defining
$$ \mu_k = \sum_{j=1}^\infty \frac{1}{j \left (j + \frac{1}{k-1}\right )}$$

\begin{theorem}[theorem 1. \cite{paturi2005improved}] Let $k\ge 3$\footnote{Here we are also using the Hertli Result\cite{hertli20143}.}, let $s(n)$ a function going to infinity. Then, for any satisfiable $k$-CNF formula $F$ on $n$ variables,
  $$\tau(F_s) \ge 2^{-(1-\frac{\mu_k}{k-1})n-o(n)}$$

  Hence ResolveSat(F,s,I) with $I = 2^{+(1-\frac{\mu_k}{k-1})n+o(n)}$ has an error probability $o(1)$ and running time $2^{-(1-\frac{\mu_k}{k-1})n-o(n)}$ on any satisfiable $k$-CNF formula, provided that $s(n)$ goes to infinity sufficiently slowly.
  \end{theorem}  

  By slowly the theorem means that $s(n)$ diverge in $o(log(n))$. Also the term $o(n)$ can be reduced as wished.

\section{Schöning WalkSAT algorithm}

  In this section we consider the Schöning WalkSAT algorithm, first introduced on 1998 \cite{schoning1999probabilistic}. At the time of publication provides the best complexity for 3-SAT, achieving $O((4/3)^{n})$, and it remained so until the Hertli result. It is also one of the most simple algorithms. The information presented here follows the discussion in the original paper \cite{schoning1999probabilistic} as well as the book \cite{schoning2013satisfiability}, also by Schöning himself.  Without further ado, we present the algorithm.\\
  

  \begin{algorithm}
    \caption{WalkSAT algorithm}\label{alg:ws}
    \begin{algorithmic}[1]
      \Procedure{\texttt{Random-LS}}{$F$, $\alpha$, $t$}
      \For{$i \in \{1,...,t\}$ }
      \If{$F\alpha=1$} \Return Satisfiable
      \EndIf
      \State Choose $C={l_1,...,l_n}\in F$ such that $C\alpha =0$
      \State Choose randomly $n\in 1,...,n$..
      \State  $\alpha \gets \alpha\circ\{l_1 \to 1\}$
      \EndFor
      \State \Return Unsatisfiable 
      \EndProcedure
      
      \Procedure{\texttt{WalkSAT}}{$F, r$}
      \State $n \gets |Var(F)|$
      \For{$i \in 1,...,r$}   
      \State $\alpha \gets $ random two-valued assignment on $Var(F)$
      \If{\texttt{Random-LS}($F, \alpha, n$) = Satisfiable} \Return Satisfiable 
      \EndIf
      \EndFor
      \State \Return Unsatisfiable
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}


  Note that we look in a random walk with as many steps as the number of variables. this parameter, in the original paper, was set to $3n$, but it was later prove that $n$ is enough. This is not really important in terms of complexity, as a constant variation does not imply a difference on its big O complexity. Nonetheless is a sensible difference on running time on industrial applications. \\

  \begin{theorem}
    WalkSAT is a correct Probabilistic Algorithm. The algorithm run on $O^*(2 (1 - \frac{1}{k})^n)$.
  \end{theorem}

  \begin{proof}
    Let's analyze this algorithms for a $k$-CNF formula $F$. As previously explained, to prove that this is a correct probabilistic algorithm, we have to prove that it has a one-sided bounded error. It is clear that, if no satisfiable assignment exists, the \texttt{WalkSAT} algorithm will always get the correct results, so the stress of the proof on bounding the probability of not finding a satisfying assignment in the case that $F$ is satisfiable.\\


    Suppose that $F$ is satisfiable, and therefore there is a satisfying assignments $\alpha_0$. Let $n=|Var(F)|$.  We will analyze the probability that $\alpha_0$ is obtanied on after $n$ iterationa of the algorithm. In order to do that we can analyze the process as a Markov Chain, that is, a sequence of random variables where the value of each variable is independent of all others except the previous one. We have secuence of random variables ($X_j$) that represents the distance $d_\mathcal{H}$ from $\alpha_0$ to a randomly drawn $\alpha$ after $j$ steps on the for loop. This random variables has an image $\{0,...,n\}$. Is easy to see that the value of the random variable $X_{i+1}$ depend only on the value on $X_i$. In particular we have a  Markov chain on a finite state space.\\

    Now let's suppose that we have an assignment $\alpha$ such that $d_\mathcal{H}(\alpha, \alpha_0) = d$. If $d=0$ we have a satisfying assignment, so there is nothing left to do. Otherwise there is a clause $C\in F$ such that $C\alpha = 0$. Let $C = (\lor_{i\in 1,..,n} l_i )$ where each $l_i$ is a literal. In the next steps of our process we will change $\alpha$ forcing a map from one $l_i$ to 1. It is clear that $\alpha_0$ maps at least one of those literals to 1. Let $p = |\{l \in C : \alpha_0(l) = 1\}|$. There, on each iteration, there is a probability of $p/k$ of decreasing the distance between $\alpha$ and $\alpha_0$ by 1 and $1-p/k$ of increasing it by 1. Therefore on the worst case we have a probability of $1/k$ to progress in the right direction. We will analyze this worst case scenario, a expecto our result to be a bound of the real one. 

\begin{figure}[H]
  \centering
  
  \begin{tikzpicture}[scale = 0.5,node distance=3cm,on grid,auto,thick] 
    % comentar para estilo básico
    \tikzstyle{every state}=[draw=myred!50,very thick,fill=myred!25,minimum size=1.2cm]
    \node[state,accepting] (q_0)   {$0$}; 
    \node[state] (q_1) [right=of q_0] {$1$}; 
    \node[state](q_2) [right=of q_1] {$...$};
    \node[state](q_3) [right=of q_2] {$n-1$};
    \node[state](q_4) [right=of q_3] {$n$};
    
    \path[->] 
    (q_1) edge node {$\frac{1}{k}$} (q_0)
    (q_1) edge [bend left]node {$\frac{k-1}{k}$} (q_2)
    (q_2) edge [bend left] node {$\frac{1}{k}$} (q_1)
    (q_3) edge [bend left]node {$\frac{1}{k}$} (q_2)
    (q_2) edge [bend left]node {$\frac{k-1}{k}$} (q_3)
    (q_3) edge [bend left]node {$\frac{k-1}{k}$} (q_4)
    (q_4) edge [bend left]node {$1$} (q_3);
    
  \end{tikzpicture}
  
  \caption{Representation of the Stocastic Process}
\end{figure}


Therefore we can see that our markov chain is denoted by the matrix:

$$
\begin{pmatrix}
  1 & 0 & 0& ... & 0\\
  1/k & 0 & (k-1)/k & ... & 0\\
  \vdots & \vdots & \vdots & & \vdots\\
  0 & ... & 1/k & 0 & (k-1)/k\\
  0 & ... & 0 & 1 & 0\\
\end{pmatrix}
$$

Now that we have all our machineri working we will define and estoy three events:

\begin{itemize}
\item The event $E_1$ is the event where $X_0 = \lfloor n/k\rfloor$, i.e.,the event of an uniformly drawn variable is at distance $\lfloor n/k\rfloor$, where $\lfloor n/k\rfloor = \max\{a \in \mathbb{N}: a \le n/k \}$. 
\item The event $E_2$ where $X_n = 0$, given that $X_0= \lfloor n/k\rfloor$.

\item The event $E_3$ is the event when $X_n = 0$
\end{itemize}


For the analysis of the probability of $E_1$ we can assume $\alpha_0 = \{ x \to 0 : x \in Var(F)\}$ without loss of generality. To drawn our assignment $\alpha$ we randomly choose a value $a_x \in \{0,1\}$ for every $x \in Var(F)$. Therefore, the number of variables mapped to 1 obtained in a random assignment follows a binomial distribution $B(n, 1/2)$. We can see that:
$$P(E_1) = P(B(n,1/2) = \lfloor n/k\rfloor) = {\displaystyle {n \choose \lfloor n/k\rfloor }  \left (\frac{1}{2}\right )^{n}} $$

Note also that $k\le n$, as by the Pigeon Hole Principle, there exists a variable $x\in Var(F)$ for every $C\in F$ such that both literals $x$ and $\neg x$ belong to $C$, and therefore the formula would be a tautology. We do not consider clauses with repeated literals.\\

For the probability of the event $E_2$, we 'walk' in the right direction at least $\lfloor n/k\rfloor$. As each step follows a Bernoulli distribution of probability $1/k$ we can see that:

$$P(E_2) =  P(B(n,1/k) \ge \lfloor n/k\rfloor) = {\displaystyle \sum _{i=\lfloor n/k\rfloor }^{n}{n \choose i }  \left (\frac{1}{k}\right )^{i}  \left (\frac{k-1}{k}\right )^{n-i}} $$


Now we can see that
\begin{equation}
  \begin{split}
    P(E_3) \ge P(E_1 \land E_2)  & = P(E_1)P(E_2) \\
    & = {\displaystyle {n \choose \lfloor n/k\rfloor  }  2^{-n}} {\displaystyle \sum _{i=\lfloor n/k\rfloor }^{n}{n \choose i }  \left (\frac{1}{k}\right )^{i}  \left (\frac{k-1}{k}\right )^{n-i}}\\
    & \ge {\displaystyle {n \choose \lfloor n/k\rfloor  }^2  2^{-n}   \left (\frac{1}{k}\right )^{n/k}  \left (\frac{k-1}{k}\right )^{n-( n/k)}  } \\
    & =^{poly \ 1.} {\displaystyle 2^{2\mathfrak{h}(1/k) n}  2^{-n}   \left (\frac{1}{k}\right )^{ n/k}  \left (\frac{k-1}{k}\right )^{n-( n/k )}  }\\
    & = \frac{ 2^{-n}   \left (\frac{1}{k}\right )^{ n/k}  \left (\frac{k-1}{k}\right )^{n-( n/k )} }{(\frac{1}{k})^{2n/k} (\frac{k-1}{k})^{2((k-1)/k)n}  }
     =\frac{1}{\left ( 2\left (1-\frac{1}{k}\right ) \right )^n} 
  \end{split}
\end{equation}

Where 1. is a direct consequence of proposition[\ref{pro:shannon}]. Now that we have bounded the probability that \texttt{Random-LS} find a satisfiable assingment, we can see that, if $r = c \left ( 2\left (1-\frac{1}{k}\right ) \right )^n$ then the probability of \texttt{WalkSAT} algorithm not finding a correct assignment conditioned on its existence is:

$$ \left ( 1-  \frac{1}{\left ( 2\left (1-\frac{1}{k}\right ) \right )^n} \right )^{\left ( c2\left (1-\frac{1}{k}\right ) \right )^n}  \approx^{2.} e^{-c}$$

On 2. we coinsder $n$ to be big enough in order to approximate to the limit of the expresion.
\end{proof}

This algorithm was fully derandomized \cite{moser2011full}. 



\section{Summary of introduced algorithms}

As a last section of this part we will summarize all SAT solver explained and classify them. Note that restricted algorithms are considered to be complete, mean that they are complete for their restricted set of Formulas.



\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|l|l|l|l|}
      \hline
      Name & Restricted & Complete & Construtive & Complexity \\ \hline\hline
      Lovasz & Yes & Yes & No & $O(n)$\\
      Constructive Lovasz & Yes & Yes & No & $O(n)$\\
      2-SAT & Yes & Yes & Yes & $O(n^2)$\\
      Horn-SAT & Yes & Yes & Yes & $O(n^2)$\\\hline\hline
      Backtrack & No & Yes & Yes & $O(2^n)$\\
      DPLL & No & Yes & Yes & $O(2^n)^*$\\
      MS & No & Yes & Yes & $O\left( 1.618^n\right )$\\
      Local Search & No & Yes & Yes & $O(1.5^n)$\\\hline\hline
      PPZ & No & No & Yes & $O(1.587^n)$\\
      PPZS(H) & No & No & Yes & $O(1.307^n)$\\
      WalkSAT & No & No & Yes & $O(1.334^n)$\\
      \hline

    \end{tabular}
  \end{center}
  \caption{\label{tab:table-name}Algorithms presented.}
\end{table}



TODO:Incluir ahora un parrafo de algoritmos no incluidos pero dignos de mención

\begin{enumerate}
\item Hoffmeister
\item Stalmark
\item Derandomized algorithms
\end{enumerate}
    
    