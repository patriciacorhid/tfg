
\chapter{Resolution Algorithms}

This chapter is fundamental as it attack the main problem of SAT: solving it. Onward we will see how it could be solved, and develop applied techniques. There are a lot of approach to this problem and they differ on it way to attack it. We have to realise that three thing are important to judge a algorithm:

\begin{itemize}
\item The simplicity: following Occam's razor, between to solution that do not appear to be better or worse, one should choose the easiest one. This solution are far more comprehensible and tend to be more variable and adaptable for our problem. We should not despise an easy solution to a complex problem only because far more difficult approach give slightly better results.

\item The complexity: and by that I mean it algorithmic ('Big O') complexity. It is important to get good running times in all cases and have a analysis of the worst cases scenario that the algorithm could have.

\item The efficiency: Some algorithms will have the same complexity as the most simple ones, but will use some plans to be able to solve the most part of the problems fast (even in polynomial time). There are some cases that would make this algorithms be pretty slow, but more often than not a trade-off is convenient.
\end{itemize}
  
  The first section will talk about combinatorics, the about some special cases, in order to continue to general ones. 

\section{Satisfiability by Combinatorics}


To get an intuition about the way that unsolvable clauses are, we gonna state some simple result about combinatorics and resolution. This will give the reader an idea of how these formulas should be. Also, these cases present some cases where we can solve the problem very efficently, although more often that not we would not provide a satisfying assignment. Nonetheless, beeing given an efficent non-constructive SAT-solver, it is possible to make a constructive solver with a running time of $n$ times the previous by assigning a literal a checking whether or not the result is still solvable. As every know general SAT-solver is exponential, a polynomial increase do not affect overall the assymptotical complexity.\\

Firstly, it is easy to break a big clause on some smaller ones, adding one another on this fashion: Suppose we got two positive integers $n,m$ such that $m < n$ a clause $x_1\vee x_2 \vee ... \vee x_n$ we could split it into two parts $x_1\vee x_2  \vee ... \vee x_{m-1} \vee y, \neg y \vee x_m \vee ... \vee x_n$. Also given the same clause with a given length $n$ we could enlarge it one variable adding $ x_1 \vee ... \vee x_n \vee y$ and $ x_1 \vee ... \vee x_n \vee \neg y$. Note that to enlarge a clause from a length $m$ to a length $n>m$ we would generate $2^{n-m}$ clauses.


\begin{proposition}
	Let $F$ be a CNF formula which has exactly k literals, if $|F| < 2^k$ then $F$ is satisfiable.
\end{proposition}
\begin{proof}
	Let $n = Var(F)$, it happens that $n > k$. For each clause $C \in F$ there are $2^{n-k}$ assignment that falsify $F$, so in total there could be strictly less than $2^k \cdot 2^{n-k} = 2^n.$ Therefore it exists an assignment that assign all variables and not falsifies the formula $F$.
\end{proof}
\begin{proposition}
	Let $F=\{C_1,...,C_n\}$ be a CNF formula. If $\sum_{j=1}^m 2^{-|C_j|}<1,$ then $F$ is satisfiable.
\end{proposition}
\begin{proof}
	Enlarging clauses the way it is explained to the maximum length $k$ ans applying the previous result.
\end{proof}

Following this idea we could define the weight of a clause $C\in F$ as $$\omega(C) = 2^{-|C|} $$
being this the probability that a uniform-random assignment violates this clause. 

\begin{corollary}
	For a formula in CNF, if the sum of the weights of the clauses is less than one then the formula is satisfiable.
\end{corollary}


\begin{proof}
For this task, we will give a probabilistic algorithm, only to prove that it will end with a big probability. Probabilistic (and heuristics) approaches to the problem would prove later on to be really useful. Let $F$ be a CNF formula regard as a clause set.
\end{proof}



\begin{definition}
Let $F$ be a CNF formula. It is said to be minimally unsatisfiable if:
\begin{itemize}
	\item $F$ is unsatisfiable.
	\item $F\backslash \{C\}$ is satisfiable $\forall C \in F$.
\end{itemize}
\end{definition}


The to following prove would be shown as done in \cite{schoning2013satisfiability}. For that resolution we will need the well known Hall marriage theorem\cite{hall2009representatives}:

\begin{theorem}[Hall marriage graph version]
  Let $G$ be a finite bipartite graph with finite sets of vertex $X,Y$. There is a matching edge cover(a cover such that every vertex only participate in one edge) of $X$ if and only if $|W| \le |N_G(W)|$.  
\end{theorem}


\begin{lemma}
Let F be a CNF formula. If for every subset $G$ of $F$ it holds that $|G|\le |Var(G)|$, then $F$ is satisfiable.
\end{lemma}

\begin{proof}
 We will Associate $F$ a bipartite graph and with $U, V$ be the two set of vertex: $U$ consists on the set of clauses and $V$ on the set of variables. By the marriage theorem every clause can be associated to a variable. Therefore we could make an assignment that take every variable associated to a clause to the value that the clause requires.
\end{proof}

This idea or neighbourhood in clause is important and curious. It defines a relation between clauses and give clauses resolution some nice graph-tools to work with.


\begin{proposition}
	Minimally unsatisfiable, then $|F| > Var(F)$.
\end{proposition}

\begin{proof}
  Since $F$ is unsatisfiable, there must be a subset $G$ such that is the maximal that satisfy $|G| > Var(G)$. If $G=F$ them the theorem is proved.


  Otherwise, be $H \subset F\backslash G$ an arbitrary subset. If $|H| > |Var(H)\Var(G)|$ then $|G \cup H| > |Var(G\cup H)|$ and $G$ would not be maximal.  Therefore $F\G$ satisfy the condition of the lemma and is satisfiable using an assignment that does not use any variable $x \in Var(G)$. As $G$ is minimally unsatisfiable $G$ is satisfiable by and assignment $\beta$. We could them define an assignment:

  \[   
\gamma(x) = 
     \begin{cases}
       \beta(x) &\quad\text{if } x \in Var(G)\\
       \alpha(x) &\quad\text{otherwise.} \\ 
     \end{cases}
   \]
this assignment would satisfy F against the hypothesis.
\end{proof}




\section{Lovász Local Lemma}
We continue to prove an interesting lemma on the theoretical analysis of satisfiability problem: the Lovász Local Lemma (LLL). This lemma was first proven on 1972 by Erdös and Lovász while they were studying 3-coloration of hypergraphs. Then it was Moser which understood the relationship between  this result an constraint satisfaction problem. The SAT could be regard as the simplest of these problems. \\

%Incluir teorema pagina 20? 

 
This section is going to be based on the works of Moser, Tardos, Lovász and Erdös as a result. As it will be shown LLL is applicable to set sufficient condition for satisfiability.  We will explain the lemma for theoretical purposes and prove the most general version, and give a constructive algorithm to solve a less general statement of the problem. The principal source of bibliography for the whole section would be Moser PhD. Thesis. \\ 
%[INCLUIR LINK A LA BIBLIOGRAFIA]


The main contribution of Moser's works to this problem is finding an efficient algorithm to find what assignment satisfies the formula, should happen that $F$ is proved satisfiable by the previous theorem. Previously only probabilistic approaches had been successful.\\


The probabilistic method is a useful method to prove the existence of objects with an specific property. The philosophy beneath this type of demonstration is the following: in order to prove the existence of an object we do not need to give the said object, instead, we could just consider a random object in the space that we consider an prove that the probability is strictly positive. Then we can deduce that an object with that property exists (if it did not probability would be 0). It is not necessary to provide the exact value, bounding it by a constant greater that 0 would be enough. \\

This technique was pioneered by Paul Erdös. The LLL takes part because is an useful tool to prove lower bounds for probabilities, allowing us to provide the result.\\

This section will follow this order:
\begin{itemize}
	\item Present the notation and general expression for the LLL.
	\item Use the result to prove an interesting property on satisfiability on CNF.
	\item Prove the general result with the probabilistic result.
	\item Provide the more concise CNF-result with a constructive algorithm.
\end{itemize}


\subsection{First definitions}

We will work here with a very specific type of formulas. Let us call a formula $F$ is in $k$-CNF
if it is in CNF and $\forall C \in F, |C| = k$.
\begin{definition}
	Let $C$ be a clause in $F$, the neighborhood of $C$, denoted as $\Gamma_F(C)$ as 
	$$ \Gamma_F(C) = \{ D \in F : D\ne C, Var(C) \cap Var(D) \ne \emptyset\}$$
	
	Analogously, the inclusive neighborhood $\Gamma_F^+(C) = \Gamma(C) \cup \{C\}$. 
\end{definition}


Further on $\Gamma$ and $\Gamma^+$ will respectively denote inclusive or exclusive neighborhood on CNF formulas or graphs


\begin{definition}
	
	Two clauses are \emph{conflicting} if there is a variable that is required to be true in one of then and to be false in the other. The graph $G_F^*$ such that there is an edge between $C$ and $D$ iff they \emph{conflict} in some variable.
	
\end{definition}
	



\begin{definition} Let $\Omega$ be a probability space and let 
$\mathcal{A} = \{A_1,...,A_m\}$ be arbitrary events in this space. We say that a graph $G$ on the vertex set $\mathcal{A}$ is a \emph{lopsidependency graph }for $\mathcal{A}$ is more likely in the conditional space defined by intersecting the complement of any subset of its non-neighbors. In others words:

\[
 P\left(  A\ \Big | \bigcap_{B\in S} \overline{B} \right ) \le P(A) \ \ \ \ \quad \forall A \in \mathcal{A},\ \forall S \subset \mathcal{A} \backslash\Gamma_G^+(A) 
\]


If, instead of requiring the event to be more likely, we require it to be independent (i.e. to be equal in probability) the graph is called \emph{dependency graph}.

\end{definition}

\subsection{Statement of the Lovász Local Lemma}

\begin{theorem}[Lovász Local Lema]\label{LLL}
	Let $\Omega$ be a probability space and let 
$\mathcal{A} = \{A_1,...,A_m\}$ be arbitrary events in this space. Let $G$ be a lopsidependency graph for $\mathcal{A}$. If there exists a mapping $\mu:\mathcal{A} \to (0,1)$ such that 
$$
\forall A \in \mathcal{A} : P (A) \le \mu(A) \prod_{B\in\Gamma_G(A)} (1-\mu(B))
$$

then $P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right ) > 0$.\\
\end{theorem}

By considering the random experiment of drawing an assignment uniformly, with the event corresponding to violating the different clauses we could reformulate this result. The weight of each clause is the probability of violating each clause. Therefore, we can state a SAT-focused result.

\begin{corollary}[Lovász Local Lema for SAT]\label{LLLS}
	Let $F$ be a CNF formula. If there exists a mapping $\mu:F\to (0,1)$ that associates a number with each clause in the formula such that 
	
	$$
\forall A \in \mathcal{A} : \omega (A) \le \mu(A) \prod_{B\in\Gamma^*_G(A)} (1-\mu(B))
$$
	then F is satisfiable.
\end{corollary}
\begin{proof}
	To prove the result it would only be necessary to show  that $ \Gamma^*$ is the lopsidependency graph for this experiment. Given $C \in F$ and $\mathcal{D}\subset F\backslash \Gamma_{G_F^*}(D)\ $(i.e. no $D \in  \mathcal{D}$ conflict with $C$). We want to check the probability of a random assignment falsifying $C$ given that it satisfies all of the clauses in $\mathcal{D}$, and prove that it is at most $2^{-|C|}$. \\ 
	
Let $\alpha$ be an assignment such that it satisfies $\mathcal{D}$ and violates $C$. We could generate new assignment from $\alpha$ changing any value on $Var(C)$, and they still will satisfy $\mathcal{D}$ (as there are no conflict) so the probability is still at most $2^{-k}$. 

% REVISAR 

\end{proof}


The result that we will prove in a constructive way will be slightly more strict, imposing the condition not only in $\Gamma^*$ but in $\Gamma^+$ 


\begin{corollary}[Constructive Lovász Local Lema for SAT]\label{LLLSC}
	Let $F$ be a CNF formula. If there exists a mapping $\mu:F\to (0,1)$ that associates a number with each clause in the formula such that 
	
	$$
\forall A \in \mathcal{A} : \omega (A) \le \mu(A) \prod_{B\in\Gamma_G(A)} (1-\mu(B))
$$
	then F is satisfiable.
\end{corollary}


In order to get a result easier to check. If $k\le 2$ the $k$-SAT problem is  polynomial solvable so we will not be interested on such formulas.

\begin{corollary}
	Let $F$ be a $k$-CNF with $k>2$ formula such that $\ \forall C \in F$ and $\ |\Gamma_F(C)|\le 2^k/e-1$ then $F$ is satisfiable.
\end{corollary}
\begin{proof}
	
	 We will try to use \ref{LLLSC}. We will define such $\mu: F \to (0,1),$$\ \mu(C)=e\cdot 2^{-k}$. Let $C_0\in F$ be an arbitrary clause.
	 
	 \[
	 2^{-k}=\omega(C)\le  \mu(C) \prod_{B\in\Gamma_F(C)} (1-\mu(B)) = e2^{-k}(1-e 2^{-k})^{|\Gamma_F(C)|}
	 \]
	 With the hypothesis
	 
	\[
		 2^{-k} \le  e 2^{-k}(1-e2^{-k})^{2^k/e-1}\]\[
		 1  \le e(1-e2^{-k})^{2^k/e-1}
	\]
	
	Being famous that the convergence of the sequence $\{(1-e2^{-k})^{2^k/e-1}\}_k$ to $1/e$ is monotonically decreasing.\\
\end{proof}



\subsection{Nonconstructive proof of \ref{LLL}}

We explain the way Erdös, Lovász and Spencer originally proved the Lemma. This material is from \cite{erdos1973problems} and \cite{spencer1977asymptotic}. The write-up presented here will resemble the one done by \cite{moser2013exact}.\\



Thorough the proof we will use repeatedly the definition of conditional probability, i.e. for any events $\{E_i\}_{i\in 1,...,r}$,

\[
P\left ( \bigcap_{i=1}^r E_1\right ) = \prod_{i=1}^rP \left( E_i \Big | \bigcap_{j=1}^{i-1} E_j\right)\\
\]

Further on this subsection we will consider  $\Omega$ to be a probability space and $\mathcal{A} = \{A_1,...,A_m\}$ to be arbitrary events in this space, $G$ to be the lopsidependency graph, and $\mu: \mathcal{A} \to (0,1)$ with such that the conditions of the theorem are satisfied. We first prove an auxiliary lemma.

\begin{lemma} \label{LemaLLL}
Let $ A_0 \in \mathcal{A} $ and $\mathcal{H}\subset \mathcal{A} $. then 
\[
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) \le \mu(A) 
\]

		
\end{lemma}

\begin{proof}

The proof is by induction on the size of $|\mathcal{H}|$. The case $H=\emptyset$ follows from the hypothesis easily:

$$ 
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) =  P(A) \le^{1.}   \mu(A) \prod_{B\in\Gamma^*_G(A)} (1-\mu(B)) \le^{2.} \mu(A) $$

Where 1. uses the hypothesis and 2. uses that $0 < \mu(B) < 1$. Now we suppose that $|\mathcal{H}|=n$ and that the claim is true for all $\mathcal{H}'$ such that $|\mathcal{H}'|<n$. We distinguish two cases. The induction hypothesis will not be necessary for the first of them
\begin{itemize}
\item When $\mathcal{H} \cap \Gamma^*_G(A) = \emptyset$ then  $	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) = 0 \le P(A)$ by definition of $\Gamma_G^*$ and $P(A) \le \mu(A)$ by definition of $\mu$.
\item Otherwise we have $A\not \in \mathcal{H}$ and $\mathcal{H} \cap \Gamma^*_G(A) \ne \emptyset$. Then we can define to sets $\mathcal{H}_A = \mathcal{H} \cap \Gamma^*_G(A) = \{H_1,...,H_k\}$ and $\mathcal{H}_0 = \mathcal{H}  \backslash \mathcal{H}_A$. 
\[
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B} \right ) = \frac{P\left ( A \cap \left ( \bigcap_{B\in \mathcal{H}_A} \overline{B} \right ) \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )
	}{P\left ( \bigcap_{B\in \mathcal{H}_A} \overline{B}  \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )}
\]

We will bound numerator and denominator. For the numerator:

\[
P\left ( A \cap \left ( \bigcap_{B\in \mathcal{H}_A} \overline{B} \right ) \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right ) \le P\left ( A \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right ) \le P(A)
\]

Where the second inequality is given by the definition of lopsidependency graph. On the other hand, for the denominator, we can define $\mathcal{H}_i := \{H_i,...,H_k\} \cup \mathcal{H}_0$.
\begin{align*}
P\left ( \bigcap_{B\in \mathcal{H}_A} \overline{B}  \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )  = & \prod_{i=1}^k P\left ( \overline{B_i} \Big| \bigcap_{B\in \mathcal{H}_i} \overline{B} \right ) \\  \ge^{3.}  & \prod_{i=1}^k \left (1-\mu(H_i)\right ) 
 \ge^{4.}  \prod_{B\in\Gamma_G^*(A)} \left (1-\mu(B)\right )
\end{align*}

Where in 3. the induction hypothesis is used, and in 4. is considering  that $H_i \in \Gamma_G^*(A)$
Considering now both parts:
\[
P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B} \right ) \le \frac{P(A)}{\prod_{B\in\Gamma_G^*(A)} \left (1-\mu(B)\right )} \le \mu(A)
\]
Where the last inequality uses the hypothesis on $\mu$.
\end{itemize}
\end{proof}


\begin{proof}[proof of the theorem \ref{LLL}]
\[
P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right ) = \prod_{i=1}^m P \left( \overline{A_i} \Big | \bigcap_{j=1}^{i-1} \overline{A_j}\right) \ge^{5.} \prod_{i=1}^m ( 1 - \mu(A_i))
\]
	Where in 5. is used \ref{LemaLLL} and since $\mu:\mathcal{A}\to (0,1)$ then $P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right )  > 0$.\\
\end{proof}


\subsection{Constructive proof of \ref{LLLSC}}

Moser\cite{moser2013exact} proves that it exists an algorithm such that it give an assignment satisfying the SAT formula, should it happen that the formula satisfies \ref{LLLS} conditions. This is no a big deal, as a backtrack would be also capable of providing the solution, given that we know its existence. Not so trivial is that it would run in $O(|F|)$. We will show the version of the algorithm shown in \cite{schoning2013satisfiability}.



\begin{algorithm}
\caption{Moser's Algorithm}\label{euclid}
\begin{algorithmic}[1]
  \State $C_1,...,C_m \gets \text{Clauses in F to satisfy, globally accessible}$
  \State $\alpha \gets \text{assignment on }Var(F)$
  \State
  \Procedure{Repair}{$\alpha, C$}
  \For{$v \in Var(C)$}
  \State $\alpha(v) = \text{random} \in \{0,1\}$
  \EndFor
  \For{j := 1 to m}
  \If {$(Var(C_j)\cap Var(C) \ne \emptyset ) \wedge (C_j\alpha=0)$}
  \State Repair($C_j$)
  \EndIf
  \EndFor
  \EndProcedure
  \State
  \State Randomly choose an initial assignment $\alpha$
  \For{j := 1 to m} 
  \If{$\alpha(C_j) = 0$}
  \State Repair($C_j$)

\end{algorithmic}
\end{algorithm}


At first sight it is not clear if it terminates. If $F$ verify \ref{LLLS} it is proved that if would end after running Repair at most  $O(\sum_{C\in F} \frac{\mu(C)}{1-\mu(C)})$


