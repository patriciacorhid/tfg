\chapter{Complexity Classes and Relevance of the Problem}
\section{Model of Computation}

In this section we discuss two computation models: Turing Machines and Circuits. We do not expect the text to be the first approximations to Turing machines, so we present a quick formal approach to the area. 

\subsection{Turing Machines}
Turing machines are arguably the epicenter of  models of computation. A Turing Machine  represents a long mechanical tape on which we are going to operate. The tape is divided in to discrete positions, such that we can see the tape as a one-dimensional array. Operating on this tape we can focus on a cell, scan its contents, overwrite  its contents or move to an adjacent cell. These operations try to resemble the process of human calculus, as done with paper and pencil when applying the long division method  for example. Formally:

\begin{definition}[Turing Machine \cite{hopcroft2007introduction}] We describe a Turing Machine as a 7-tuple $M=(Q, \Sigma, \Gamma, \delta, q_0, B, F)$ whose components have the following meanings:
  \begin{itemize}
  \item $Q$ the finite set of \emph{states} of the finite control.
  \item $\Sigma$ the finite set of \emph{input symbols}.
  \item $\Gamma$ the finite set of \emph{tape symbols}. $\Sigma$ is always a subset of $\Gamma$.
  \item  $\delta: Q\times \Gamma \to Q\times\Gamma\times\{L,R\}$ the transition function.
  \item $q_0$ the \emph{start state}.
  \item $B$ the \emph{blank symbol}.
  \item $F$ the \emph{accepting states}.
  \end{itemize}

  A configuration of a Turing machine is a triplet $C=(q,u,v)$ where $q\in Q$, $u,v\in \Gamma^*$. A configuration is accepting if $q\in F$.
\end{definition}

A configuration should be understand as a state of the machine, where $q$ is the current state, $u$ the part of the tape left to the cell on which we focus and $v$ is the part of the tape right to the cell we focus, starting on it.\\

As a brief note before going on, we have not define the empty word yet, as in propositional logic is not a valid formula so it lacks our interest until now. We will note the empty word as $\epsilon$ and would consist of an empty sequence of symbols. We can  now define a relation between configurations:

\begin{definition}\label{def:paso}
  Let $M$ be a Turing Machine (TM) and $C=(q,u,v)$, $C'=(q,u,v)$ be two configurations of $M$. We say that $C\vdash C'$ if there is a transition $\delta (q,v_1) = (q', b, D) $ with $D\in{L,R}$ and:
  \begin{itemize}
  \item if $D=L$, then if $u=u_1...u_n$ and $v = v_1...v_m$, it should happen that $u' = u_1...u_{n-1}$ and $v' = u_n bv_2...v_n$ with two exceptions:
    \begin{itemize}
    \item if $u=\epsilon$ then $u' = \epsilon$ and $v' = bv_2...v_n$
    \item if $v = v_1$ and $b =B$ then $u'=u_1...u_{n-1}$ and $v' = u_n$.
    \end{itemize}

  \item if $D=R$, then if $u=u_1...u_n$ and $v = v_1...v_m$, it should happen that $u' = u_1...u_{n}b$ and $v' = v_2...v_n$ with two exceptions:
    \begin{itemize}
    \item if $u=\epsilon$ then $u' = b$ and $v' =v_2...v_n$
    \item if $v = v_1$ and $b =B$ then $u'=u_1...u_{n-1}$ and $v' = \epsilon$.
    \end{itemize}
  \end{itemize}

  Note that on both cases the two exceptions can be given simultaneously. We say $C\vdash^* C'$ if there exists a finite sequence $\{C_i\}_{i\in 1,...,n}$ such that $C_1 = C$, $C_n=C'$ and $C_i\vdash C_{i+1}$ for every $i\in 1,...,n-1$.  
  \end{definition}

  We now describe the use of Turing Machine: solving problems. We consider that Turing Machines solve both decision and function problems. Lets start by explaining how a Turing Machine solves a decision problem.

  \begin{definition}
     Let $M$ be a Turing Machine. We say that $u\in\Sigma^*$ is \emph{accepted} by $M$ if there exists a final configuration $C$ such that $(q_0,\epsilon,u)\vdash^* C$. The language \emph{accepted} by $M$ denoted as $L(M)$ is the collection of all words accepted. We say that $M$ \emph{decides} a language $L$ if $L$ is the language accepted by $M$.
  \end{definition}

  With regard to solving a function problem, the intuitive idea is that we write the input on the tape and after some computations we have written on the tape a word that is related to the input one. Formally:


  \begin{definition}
    Let $R\subset\Sigma^*\times \Sigma^*$. A Turing Machine $M$ \emph{compute} $R$ if for every $u\in \Sigma^*$ there is an accepting configuration $C=(q',v,v')$ of $M$  such that $(q,\epsilon,u)\vdash C$ and $(u,vv') \in R$. A Turing Machine $M$ computes a function problem defined by $R$ if it computes $R$.
  \end{definition}


%  Among the turing machines, the universal turing machine should be highlighted.It was Turing who elucidated that a turing machine could be built such that by giving as an input the description of a Turing machine and a word would act as the described machine when receiving that word as input.
  
\subsection{Circuits}

The circuit model provide another computation model that is naturally related to propositional logic. We define it briefly in order to prove some results on the area. Circuits are the computation model that is used in practice. One of the most important area of industrial application of SAT is circuit verification. This is going to be discussed further on AÑADIR CAPITULO.

\section{Reductions}
A language $L1$ is Turing reducible to a language $L2$ iff there is an oracle Turing machine $M$ that accepts $L1$, where$M$is allowed to make membership queries of the form $x\in L_2$, which are correctly answered by an“oracle” for $L2$. Later, the more restricted notion of many-one reducibility ($\le_m$)was introduced and defined as follows.


\section{Complexity Classes}
\label{sec:complexity}

In this section we are going to define what a complexity class is and then we are going to discuss some results of the complexity of the SAT-problem. We use as principal reference \cite{arora2009computational}.

\subsection{Deterministic complexity}
\label{sub:detcomp}
There are different approaches as how to measure the complexity of given algorithm. We will focus primarily on \emph{worst-case time complexity}. We will also introduce \emph{worst-case space complexity} and provide some results.

\begin{definition}
  Let $g:\mathbb{N}^\mathbb{N}$, the:
  $$O(g) = \left\{ f\in \mathbb{N}^\mathbb{N} :\ \exists N,C \in \mathbb{N} : f(n) \le Cg(n) \ \forall  n \ge N\right\}$$
\end{definition}

\begin{definition}
  Let $f:\mathbb{N}^\mathbb{N}$.
\begin{enumerate}
  \item We let \emph{TIME}$(f)$ (resp. \emph{FTIME}$(f)$) be the set of all decision  problems (resp. function problems) that can be decided (resp. computed) by a Turing machine $M$ using less that $g(n)$ steps where $g\in O(f)$ and $n$ is the number of characters of the input.  
  \item We let \emph{SPACE}$(f)$ (resp. \emph{FSPACE}$(f)$) be the set of all decision problems (resp. function problems) that can be decided (resp. computed) by a Turing machine $M$ using less that $g(n)$ cells where $g\in O(f)$ and $n$ is the number of characters of the input.  
  \end{enumerate}

\end{definition}

This definition let us a great tool in order to define collections of problems, and to compare them. We introduce some classes.
\begin{definition}
  Let $\Omega$ be a set of endofunctions of $\mathbb{N}$.
  \begin{enumerate}
  \item We have a homonym complexity class $\Omega$:
  $$\Omega =  \bigcup_{f \in \Omega} TIME(f) $$
  \item We have an associated function complexity class $FA$:
    $$F\Omega =  \bigcup_{f \in \Omega} FTIME(f)$$
  \item We have an associated space complexity class $FA$:
    $$\Omega SPACE =  \bigcup_{f \in \Omega} SPACE(f)$$  \end{enumerate}
\end{definition}


Although we have define the complexity classes as collections of decision problems, we can also consider consider a language to be in a class $C$ if its associated decision problem is. During the study we will focus on decision problem complexity classes. Nonetheless ties between this classes and its associated function complexity classes are strong and will be pointed out.\\

In general the notation of complexity classes is additive. Therefore is we want to represent the function space complexity class associated to $\Omega$ we denote that by $F\Omega SPACE$.\\


Arguably the most important of the complexity classes is P. We define P as the set of all polynomials. Therefore we have an homonym complexity class P. We can justify the importance of this class the Feasibility Thesis.

\begin{thesis}[\cite{cook2006p}]
A natural problem has a feasible algorithm iff it has a polynomial-time algorithm.
\end{thesis}

That is, only problems with polynomial time complexity are able to be computed, as otherwise their complexity make them not viable, i.e., not computable in a coherent time. The idea of problems being not feasible is the basis of some of the most important conceptions of modern Computer Science. In particular in modern cryptoanalysis the most common encoding technique (private keys protocols) are based upon the conception that no one would be able to solve the presented problem without some added information in a coherent time, independently of their computing capabilities.\\




\subsection{Non-Deterministic Complexity}

Analogous to the concept of Turing Machine, another recurrent idea in computation is the concept of non-deterministic computing. These models allow an algorithm to react different to the same input. These models are useful as there as they encapsulate various problem of interest and give upper bound to the deterministic complexity. In order to formalize this concept we will define non-deterministic Turing Machines.\\

Intuitively, a non-deterministic Turing Machine is a Turing Machine that, at any point on its computation, can choose from several different 'paths' to compute. This choice is made in a non-deterministic manner, that is, it is not known what the result will be until the computation is done.

\begin{definition}\label{def:NDTM} We describe a Non-Deterministic Turing Machine (NDTM) as a 7-tuple $M=(Q, \Sigma, \Gamma, \delta, q_0, B, F)$ whose components have the following meanings:
  \begin{itemize}
  \item $Q$ the finite set of \emph{states} of the finite control.
  \item $\Sigma$ the finite set of \emph{input symbols}.
  \item $\Gamma$ the finite set of \emph{tape symbols}. $\Sigma$ is always a subset of $\Gamma$.
  \item  $\delta\subset (Q\times \Gamma)\times( Q\times\Gamma\times\{L,R\})$ the transition relation.
  \item $q_0$ the \emph{start state}.
  \item $B$ the \emph{blank symbol}.
  \item $F$ the \emph{accepting states}.
  \end{itemize}

  A configuration of a Turing machine is a triplet $C=(q,u,v)$ where $q\in Q$, $u,v\in \Gamma^*$. A configuration is accepting if $q\in F$.
\end{definition}

Note that there exists $d_0 \in \mathbb{N}$ such that:
  $$ \left| \left \{ (q,\gamma',D) \in  ( Q\times\Gamma\times\{L,R\})\ :\ ( (p,\gamma), (q,\gamma',D) ) \in \delta \right \}\right | \le d_0.$$
  Such $d_0$ is called \emph{branching factor} of $M$.\\

By opposition, the previously defined Turing machines are considered Deterministic Turing Machines(DTM). The definition of Non-Deterministic Turing Machine is adapted from \cite{hopcroft2007introduction}. \\
  
The definition [\ref{def:paso}] holds for non-deterministic Turing machines, with the consideration that now instead of finding the image of a function we have to find a related triple. Using analogous notions we define when a problem is decided/computed by a Non-deterministic Turing Machine. A Non-Deterministic Turing machine can be regarded as a tree. In this tree we assume that a branch divides when a decision has to be made. Once this tree is built, we can assume that the execution of our TM will be the progression of one of the possible paths of the tree.\\

\begin{definition}
  Let $f: \mathbb{N}\to  \mathbb{N}$.
\begin{enumerate}
  \item We let \emph{NTIME}$(f)$  the set of all problems that can be decided/computed by a non-deterministic Turing machine $M$ using less that $g(n)$ steps where $g\in O(f)$ and $n$ is the number of characters of the input.  
  \item We let \emph{NSPACE}$(f)$  the set of all problems that can be decided/computed by a non-deterministic Turing machine $M$ using less that $g(n)$ cells where $g\in O(f)$ and $n$ is the number of characters of the input.  
\end{enumerate}
\end{definition}
  
As with the previous definitions this notation is additive. We can now prove a simple result that relates the concepts defined so far.

\begin{theorem}
  Let $f: \mathbb{N}\to  \mathbb{N}$.
  \begin{enumerate}
  \item $SPACE(f) \subset NSPACE(f)$,\\
    $TIME(f) \subset NTIME(f)$.
  \item $NTIME(f) \subset SPACE(f)$.
  \item $NSPACE(f) \subset TIME(K^{ \log(n)+ f(n)} )$.
  \end{enumerate}
\end{theorem}
\begin{proof}\hfill
  \begin{enumerate}
  \item Every DTM is a NDTM if we consider the transition function as a relation.
  \item For every $L\NTIME$, let $M$ be its associated NDTM and let $d_0$ be the branching index of $M$. Any sequence of choice can be written as a number of length $f(n)$ in $d_0$-base. Iteratively execute all options possible, and accept if any of them accepts. Reject otherwise.
  \item Let $L \in NSPACE(f)$ and $M$ be its associated NDTM. We use the Achievement Method. This method is based on building a network in which the nodes are the possible configurations of a Turing Machine, and the arcs connect configurations such that you can get from one to the other in one step of calculation. The maximum number of configuration is $$|Q||\Gamma|^{f(n)} (n + 1) = |Q||\Gamma|^{f(n)+\log(n + 1)} \in O (K^{f(n)+\log(n)}).$$
Checking if a word is accepted is the same as checking if from a node in a network you can get to an acceptance node. This is checked in quadratic time against the number of nodes, thus obtaining the desired result. As this can be done in quadratic time, the result is proved.
  \end{enumerate}
\end{proof}

Analogous with what we done with deterministic complexity, we can define non-deterministic complexity classes.

\begin{definition}
  Let $F$ be a set of endofunctions of $\mathbb{N}$. We have a complexity class $NF$ defined:
  $$NF =  \bigcup_{f \in F} NTIME(f) $$
\end{definition}

At this point we can state a corollary of high importance for the mathematical community.

\begin{corollary}
  $P \subset NP$.
\end{corollary}

This corollary let itself to a simple doubt: is that inclusion strict? This problem was first stated independently by Stephen Cook and  Leonid Levin in 1971\cite{cook2006p} and is one the Clay Math Institute Millennium Problems.\\

Another characterization of NP is by \emph{verfiers}. Given a language that might not be in P, it may be possible to decide the membership of an element to that language when a \emph{proof} is given. Formally:

\begin{proposition}
  A language $L\subset A^*$ is in NP if and only if there is an relationship $R\subset A^*\times A^*$  computable in polynomial time and a polynomial $p$ such that
  $$L = \{x \in A^*:\exists y \in A^* \text{ with } |y | \le p(|x|), R(x, y) = 1\}$$
\end{proposition}
\begin{proof}\hfill
\begin{itemize}
\item[\fbox{$\Rightarrow$}] If $L\subset A^*$ is in NP, there is a NDTM $M$ that decides $L$ in $O(p)$, with $p$ a polynomial. For every $x\in L$ we have that $M$ accepts $x$ after at most $p(|x|)$ decisions. Let $<d_{x,i} : 1 \le i \le p(|x|)>$ be the word that codify such decisions in the alphabet $A^*$. We can define the relationship:
  $$R = \{(x,y)  : x \in L, y = <d_{x,i} : 1 \le i \le p(|x|)>\}$$
  We can compute such relationship with a DTM $M'$ that works as $M$, but at each iteration $i$ make the decision $d_i$. 
\item[\fbox{$\Leftarrow$}] We define a NDTM $M$ that works like follow, for an input $|x|$:
  \begin{enumerate}
  \item Non-deterministically choose a word $y$ that $|y|\le p(|x|)$. The selection of $y$ is done in polynomial time as each symbol takes one step.
  \item Accepts if $(x,y)\in R$. Rejects otherwise. This can be done in polynomial time as $R$ can be computed in polynomial time.
    \end{enumerate}
  \end{itemize}
  As both steps are computed in polynomial time $M$ runs in polynomial time.  
\end{proof}

This proposition is analogous with functions problems. This proposition give another meaning to the Pvs.NP problem. We can consider P to be the class of efficiently doable problems, whether NP is the class of efficiently checkable problems.




\section{Completeness}

In this subsection we introduce the notion of completeness. This area was developed in late 1960s and early 1970s parallel by researchers on US and URSS, at during the cold war. The first point in the develop of this theory and most important result from the point of view of this text is the Cook-Levin theorem [\ref{ref:cooklevin}], which highlights the theoretical relevance of SAT. The notion of completeness was introduced to the western world first by Cook \cite{cook1971complexity} although the term was coined later. 

\subsection{Definition}



\subsection{Cook-Levin Theorem}
At this point we introduce the main theorem of this section. The theorem was first on \cite{cook1971complexity}, although some ideas were first spoken about by Leonid Levin on 1969, the formal explanation of the result was first written by Stephen Cook. Due to our lack of knowledge of the Russian language we did not research Levin's original result, although we would like to point out that he did this research on the advice of the famous mathematician Kolmogorov. Also, later in his life, Levin emigrated to the US and was able to share his knowledge with the Western block\cite{cvlevincook}.\\


The theorem proves that SAT is NP-Complete, and was at the time the first completeness result provided. To this day virtually all proofs of problems to be NP-Complete are done by using either this result of the provided idea of it. For this theorem Cook received on 1982 the Turing Award, the highest honor conceded in the area of Computer Science. Without further ado with introduce the theorem, along with a brief lemma.



%TODO: Preguntar Serafín sobre este resultado. In fact, instead of proving the result with the today-standard 3CNF satisfiability he proved the result for DNF tautology problem. Cook original result we can see that the modern standard statement of the theorem is an adaptation of the original statement done in theorem 1\\ \cite{cook1971complexity}. The proof showcased in this paper is the one followed to this dated by most text, with only a few adaptations. 

\begin{lemma}
  For a set $A = \{a_1,...,a_n\}$ of Propositional Logic variables, we can define a set of clauses $\mathcal{C}(A)$ such that, for an assignment $alpha$ we have   $\mathcal{C}(A)\alpha = 1$ iff $\alpha$ maps to 1 one and only one of the variables in $A$.
\end{lemma}
\begin{proof}
  We define $$\mathcal{C}(A) = D\cup \left (  \cup_{i,j \in 1,..,n} D_{i,j} \right )$$ where:
  \begin{equation}
    \begin{split}
      D & = (a_1 \vee ... \vee a_n ),\\
      D_{i,j} & = (\neg a_{i} \vee \neg a_{j} ),
\end{split}
\end{equation}

From $D$ it follows that $\alpha$ satisfy at least one variable, and from $D_{i,j}$ that $\alpha$ satisfy at most one.
\end{proof}


\begin{theorem}[Cook-Levin theorem]\label{ref:cooklevin}
 SAT is NP-Complete.
\end{theorem}
\begin{proof}
  Suppose that a language $L\subset A^*$ is accepted by a NDTM $M$ within time $O(q(n))$, where $q(n)$ is a polynomial. Given an input  $w\in A^*$ of $M$, we construct a CNF-Formula $\phi(w)$ such that $\phi(w)$ is satisfiable iff $M$ accepts $w$. As the construction is done in polynomial time, we have a polynomial reduction of the problem.
  
  Suppose that $\Gamma = \{\gamma_0,\gamma_1,...,\gamma_l\}$ is the tape alphabet  of $M$ with $B=\gamma_0$ .$Q=\{q_1,..,q_s\}$ is the set of states with $q_0$ the initial state. $\delta_0$ is the branching factor of $M$. $T(n)$\footnote{$M$ is in NP therefore $M$ is in PSPACE and as a consequence $T(n)$ exists } is the polynomial bound of the number of cells used by $M$. We define $T=T(|w|)$

  \begin{itemize}
  \item Firstly, we define the variables of $\phi(w)$.
    \begin{enumerate}
    \item The variables $\{p_{s,t}^i : 1 \le i \le l,\ 1 \le s,\ t \le T \}$. These variables represents (semantically) if tape cell $s$ at step $t$ contains the symbol $\gamma_i$.
    \item The variables $\{q_{t}^i : 1 \le i \le l,\ t \le T \}$ that represents if at step $t$ the machine is in state $q_i$.
    \item The variables $\{S_{s,t} :  1 \le s,\ t \le T \}$. These variables represents if tape cell $s$  is being scanned at step $t$.
    \item The variables $\{o_{d,t} :  1 \le d \le \delta_0,\ t \le T \}$. These variables represents that decision $d$ is made at step $t$.
    \end{enumerate}

  \item Once we have the variables, we define the clauses of $\phi(w)$.
    \begin{enumerate}
    \item For $t\le T$, the clauses $\mathcal{C}(S_t)$ with $S_t = \{S_{i,j}^t : i,j \le T\}$  that ensures that at time $t$ one and only one cell is being scanned.
    \item  For $t\le T$, the clauses $\mathcal{C}(C_{t})$ with $C_t = \{p_{s,t}^{i} : 1\le i \le l, 1 \le s \le T\}$ with  that ensures that one and only one symbol is at each step of the machine at each time.
    \item  For $t\le T$, the clauses $\mathcal{C}(O_{t})$ with $O_t = \{o_{d,t}^{i} : 1\le d \le \delta_0\}$ with  that ensures that one and only one decision is made at each step of the machine at each time.
    \item For $t\le T$, the clauses $\mathcal{C}(D_{t})$ with $D_t = \{q_t^{i} : 1\le i \le l\}$ with  that ensures that at time $t$ one and only one state is active.
    \item If $w = <\gamma_{i_1}...\gamma_{i_k}>$ we add the clauses $(p_{j,0}^{i_j})$ for $j$ in $1,...,k$. These clauses ensure the initial state. Also, for $j$ in $k+1,....,T$ we add $(p_{j,0}^0)$.  
    \item For each pair state-symbol $(q_k,\gamma_d)$ we let $\{(q_{k_1} , \gamma_{d_1} , m_1 ), ... , (q_{k_l} , \gamma_{d_l} , m_l)} \subset Q \times \Gamma \times \{-1,1\}$ be the set of related transitions. We add the clauses:

    \[
    (\neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor q_{t+1, k_e})
    \]
    \[
(      \neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor p_{s,t+1}^{d_e})
    \]
    \[
(      \neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor S_{s+m_e, t+1})
    \]

    with $0 \le s,t \le T$, $0 \le d \le  \delta_0$. 

    If the set of related transitions is empty we add
    \[
    (\neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor q_{t+1, k})
    \]
    \[
(      \neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor p_{s,t+1}^{d})
    \]
    \[
(      \neg s_{s,t} \lor \neg q_t^i  \lor p_{s,t}^d \lor \neg o_{t, e}  \lor S_{s, t+1})
    \]

    with $0 \le s,t \le T$, $0 \le d \le  \delta_0$.
    
    This represent the machine's operation.
  \item In order to maintain the tape unchanged in the cells where no operation is done we add:

    $$\neg s_{t,s} \lor \neg p_{s,t}^i \lor  p_{s,t+1}^i  $$
    for $0 \le s,t \le T$, $0 \le i \le l$.
  \item Suppose $\{q_{i_1},...,q_{i_k}\}$ is the set of final states. Then we add the clause:
    $$ (q_{i_1}^T \lor ... \lor q_{i_k}^T)$$

    That represents the necessity of the machine to end on a final state in order to accept $w$.
    \end{enumerate}


    The equivalence of the Problems is clear from the way the machine is built. The clauses reproduce exactly how the machine works for the entry word $w$ and include a clause that indicates that the word is accepted. If in the end (time $T$) the clause $ (q_{i_1}^T \lor ... \lor q_{i_k}^T)$ can be satisfied, then a state of acceptance can be reached. Then the word is accepted if and only if all are possible satisfy all the clauses.\\

As all operation described to express the clause can be done within polynomial time, we have found a P-reduction of any NP problem to SAT.
  \end{itemize}

  
  
\end{proof}



%After some time dealing with the essence of SAT solving we can imagine this theorem to have a pretty simple idea. Any NDTM consists of a series of decision taken over time , and in fact that pretty much what SAT solving do, as we show in chapter 4-6.

\begin{enumerate}
  \item TODO:
  \item To study the complexity of the SAT problem and its variants so far.
  \item Include graph automorphism complexity.
  \item SSAT es PSPACE Completo
  \item Resultado craig interpolant
  \item CoNPComplete result
  \item Moreover, as checking whether an assignment is autark is linear on the number of clauses, then this make the autark-finding problem NP-Complete(NP-C further on).
  \end{enumerate}

\subsection{Exponential Time Hypothesis}
\label{hyp:exponential_time}
In this subsection we will introduce the result, shown first on \cite{impagliazzo2001complexity}. This result states simply that no sub-exponential time algorithm can be found for 3-SAT. This hypothesis, although widely accepted, is still unproven. Formally:

\begin{definition}[ETH]
  For $k\ge 3$, lets define:
  $$s_k=\inf\{\delta: \text{there exists a } O(2^{\delta n})\ \ k-\text{ SAT solver.}\}$$
  It is claim that $s_k>0$.
\end{definition}

This result has some equivalent formulations.

\begin{proposition}[theorem 1. \cite{impagliazzo2001complexity}]
  The following statements are equivalent:
  \begin{enumerate}
  \item ETH: for all $k\ge 3$, $s_k > 0$.
  \item For some $k$, $s_k \ge 0$.
  \item $s_3 \ge 0$.

  \end{enumerate}
  \end{proposition}

This theorem is prove making use of Sparsification Lemma, which is based in turn on the ideas of critical and forced variables. By the time of the publication of the article, Zane has already worked with these ideas for the development of the PPZ algorithm\cite{paturi1997satisfiability}. Although we are not going to prove this result, we are going to present this ideas when analyzing the Paturi-Pudlák-Zane Algorithm[\ref{subsec:PPZ}].
 
This claim is harder that $P\ne NP$, as it not only declares that SAT is not polynomial time, but neither is sub-exponential time. 

\section{Postponed results}
In this section we will develop some result that where intentionally postponed, in order to talk about them once complexity classes has been introduced, although they belong thematically to previous sections. 
\subsection{Tseitin Theorem}
Now that we are able to talk about efficiency is time to talk about an interesting, anticipated result. If we remember Lindenbaum algebra[\ref{def:linden}] we have defined a quotient space on the formulas in terms of satisfiability. In order to solve GSAT, we are not in need of solving all formulas.  Instead we can learn how to solve a language of formulas $\mathsrc{F}$ such that for every class $[\phi_1]\in Form/\sim$ there  is a formula $f\in\mathsrc{F}$ such that $f \in [\phi_1]$. Also, we will need a method that allow us to find such $f\in\mathsrc{F}$ given any element of $[\phi_1]$.We want to prove that SAT is a language that satisfy this restrictions. The naive approach to the problem is straightforward:\\

\begin{proposition}
  There is a $CNF$ formula in each equivalence class. Moreover, given a function $f\in Form$ we are able to find an equivalent $CNF$ formula.
\end{proposition}
\begin{proof}
 Given $\phi_1 \in Form$ we make the truth table of $\phi_1$. Two formulas are in the same equivalent classes if, and only if, they share the same truth table. \\

  We can generate a $CNF$ formula that has the same table this way: for every row $[x_1\to a_1,...,x_n\to a_n]$ ($x_i$ variables, $a_i\in \{0,1\}$) that falsify $\phi_1$ we add a clause $(z_1\vee ... \vee z_n)$ with $z_i = x_i$ if $a_i = 0$ and  $z_i =\neg x_i$ if $a_i = 1$.
\end{proof}


  This method is interesting as it show the truth table, as the collection of all two-valued assignments$\alpha$ such that $Var(\alpha) = \phi_1$. Nonetheless is not a method that should be considered useful, as is has exponential time. Tseitin theorem provide us with a solution to this problem that run in polynomial time. We will need a lemma first:

  \begin{lemma}
    For every \emph{SAT} formula there is an associated circuit.
  \end{lemma}
  \begin{proof}
    Every operator can be seen as a gate and every variable as an input.\\
  \end{proof}
  
  \begin{theorem}[Tseitin \cite{tseitin1983complexity}] \label{the:Tseitin}
    There is a 3-CNF formula on each equivalent class. Moreover, given an element $F$  there is a equivalent formula $G$  in 3-CNF which could be computed in polynomial time. 
  \end{theorem}

  \begin{proof}
    We will show that for every circuit with $n$ inputs and $m$ binary gates there is a formula in \emph{3-CNF}  that could be constructed in polynomial time in $n$ and $m$. Then, given a formula we will work with it considering its associated circuit.\\
    
    We will construct the formula considering variables $x_1,...,x_n$ that will represent the inputs and $y_1,...,y_m$ that will represents the output of each gate. 

    $$ G = (y_1) \wedge \bigwedge_{i=1}^m (y_i \iff f_i(z_{i,1},z_{i,2}))$$

    Where $f_i$ represents the formula associated to the $i$-gate, $z_{i,1},z_{i,2}$ each of the two inputs of the $i$-gate, whether they are $x_-$ or $y_-$ variables. This formula is not \emph{3-CNF} yet, but for each configuration being $f_i$ a Boolean operator there would be a \emph{3-CNF} equivalent.

    \begin{itemize}
    \item $z \iff( x \vee y )  = \neg  ( z \vee  x \vee y    ) \vee (z \wedge ( x \vee y )  ) = \neg  ( z \vee  x \vee y    ) \vee (z \wedge x)  \vee (z \wedge y ) =$\\$= ( \neg  z \wedge  \neg  x \wedge \neg   y    ) \vee (z \wedge x)  \vee (z \wedge y )  =$$
      (\neg  z \vee (z \wedge x)  \vee (z \wedge y ))  \wedge  
      (\neg  x \vee (z \wedge x)  \vee (z \wedge y )) \wedge
      (\neg  y \vee (z \wedge x)  \vee (z \wedge y ))   =
      (\neg  z \vee x  \vee y )  \wedge  
      (\neg  x \vee z  ) \wedge
      (\neg  y \vee z ) $   
    \item $z \iff( x \wedge y ) = \neg ( z \vee ( x \wedge y )) \vee (z \wedge ( x \wedge y )) = (z\wedge x \wedge y ) \vee  (\neg  z\wedge \neg  x \wedge \neg  y )  =$\\$\ \ \ ((z\vee  (\neg  z\wedge \neg  x \wedge \neg  y )  ) \wedge (x \vee  (\neg  z\wedge \neg  x \wedge \neg  y )  ) \wedge (y\vee  (\neg  z\wedge \neg  x \wedge \neg  y )  ) ) = (\neg  x \vee z) \wedge (\neg  y \vee z ) \wedge (\neg  z \vee x ) \wedge (\neg  y \vee x ) \wedge(\neg  z\vee y )\wedge (\neg  x\vee y )$
      
    \item $z \iff( x \iff y ) =  \neg ( z \vee ( x \iff y ) ) \vee (z \wedge ( x \iff y ) = \neg ( z \vee (\neg  x \wedge \neg  y) \vee (x \wedge y)) \vee (z \wedge(\neg  x \wedge \neg  y) \vee (x \wedge y))  )=(\neg  z \wedge \neg  (\neg  x \wedge \neg  y) \wedge \neg  (x \wedge y)) \vee (z \wedge(\neg  x \wedge \neg  y) \vee (x \wedge y))  )=(\neg  z \wedge  (x \vee  y) \wedge (\neg  x \vee \neg  y)) \vee (z \wedge(\neg  x \wedge \neg  y) \vee (x \wedge y))  )=z \vee ( \neg  x \wedge \neg  y) = (\neg x \vee \neg y \vee z) \wedge (\neg x \vee \neg z \vee y) \wedge (y \vee z \vee x) \wedge (y \vee \neg y \vee x) \wedge (\neg z \vee z \vee x) \wedge (\neg z \vee \neg y \vee x)$
      
    \item $z \iff( x \oplus y ) =  z \iff(\neg  x \iff y )  $	

    \end{itemize}
    In the last item we use the third one.
    
  \end{proof}

  The fact that they are reachable on polynomial time is important because it means it could be done efficiently. Should this be impossible it will not be of much relevance in practice, as we yearn to solve this problem as efficient as possible (in fact, as polynomial as possible). This result implies that if we know how to solve $3$-SAT we know how to solve GSAT.


  \subsection{Tautologies Revisited}

  TODO: include information about craig interpolants.
  \begin{proposition}
    Given a tautology $F \to G$, there exists a formula $I$ such that $Var(I) = Var(F)\cap Var(G)$ and both $F\to I$ and $I \to G$ are tautologies. A polynomial algorithm to solve this problem is not known. 
  \end{proposition}

  \begin{proof} Let $\{x_1,...,x_k\} = Var(F)\cup Var(G)$ then we will build $I$ by defining its truth table in the following way: Given an assignment $\alpha$:
    \[   
      I\alpha = 
      \begin{cases}
        1&\quad\text{if $\alpha$ could be extended to an assignment that \emph{satisfies} $F$}, \\
        0&\quad\text{if $\alpha$ could be extended to an assignment that \emph{nullifies} $G$},\\
        * &\quad\text{otherwise.} \\ 
      \end{cases}
    \]

    Where * mean that it could be either 0 or 1.  This is well defined because if for an arbitrary $\alpha$ it happens that $G\alpha = 0$ then $F\alpha = 0$.

    For every assignment $\beta$ such that $Var(\beta) = Var(F)\cup Var(G)$ then if $\beta(F) = 1$ then $\beta(I) = 1$ so $F \to I$  is a tautology. Similarly it can not happen that $I\beta = 1 $ and $G\beta = 0$, because the second will imply that   $I\beta = 0$.\\

    For the last part we will refer to the paper on the topic by: \red{TODO}
  \end{proof}

  \subsection{From non-constructive to constructive}
  \label{sub:fromnon}
  This subsection we explain how a constructive SAT-solver can be made from a non-constructive SAT-solver without changing is asymptotic time complexity, assuming true the exponential time hypothesis[\ref{hyp:exponential_time}]

  \begin{proposition}
    Let $\phi$ be an oracle that decides SAT in $O(\varphi(n+m))$ where $n$ is the number of variables and $m$ the number of clauses. Then we can make an algorithm  that computes FSAT on $O((n(\varphi(n+m))+m)$ 
  \end{proposition}
  \begin{proof}
    We will iteratively expand a partial assignment $\alpha$. $\alpha$ initially maps all variables to $\upsilon$. The procedure take as input a CNF formula $F$. The algorithm that solve FSAT is described in [\ref{alg:constructive}]. It is based in the notion that, if  $F$ is satisfiable, either $F\{x=1\}$ or $F\{x=0\}$ is satisfiable. We are able to explore the variable lineally being sure that we are always assigning the correct value to each variable. 

    
    \begin{algorithm}
  \caption{FSAT routine}\label{alg:constructive}
  \begin{algorithmic}[1]

    
  \Procedure{Solver}{$F$}
  \State $F_0 \gets F$
  \State $\alpha \gets$ empty partial assignment.
  \State
    \For{$x\in Var(F)$}
    \If{$\phi(F_0\{x=1\})$}
    \State $\alpha += \{x = 1\}$
    \State $F_0 \gets F_0\{x=1\}$
    \Else \If{$\phi(F_0\{x=0\})$} 
    \State $\alpha += \{x = 0\}$
    \State $F_0 \gets F_0\{x=0\}$
    \Else
    \State \Return Unsatisfiable
    \EndIf
    \EndIf
    \EndFor
    \State \Return $\alpha$
  \end{algorithmic}
\end{algorithm}

Let's analyze the complexity of this algorithm. We make at most $n$ repetitions of the for loop, and on each repetition we call $\phi$ and assign a variable in a formula.Therefore the procedure run in $O(n\varphi(n+m)+m)$
\end{proof}

Assuming ETH we assume that $\phi$ is exponential in time, an therefore asymptotic complexity $O(\varphi(n+m))$ is the same as $O(n(\varphi(n+m)+m))$, so, until $ETH$ is proved wrong we can consider SAT and FSAT as being equal in complexity. On this text we will only deal with non-constructive  solver on the combinatorics section[\ref{sec:combu}].



